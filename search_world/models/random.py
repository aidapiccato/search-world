import numpy as np
from itertools import product
import copy
import matplotlib as mpl
import matplotlib.pyplot as plt 

# TODO: Create agent base class with abstract methods; render, reset, __call__
# TODO: Create class for Maze states
# TODO: Make state space an accessible property of environment; part of wrappers

class BeliefState(object):
    def __init__(self, env):
        self._env = env

    def mls(self):
        """Returns most likely state, breaking ties randomly. 
        """
        return np.random.choice(len(self._state_space), p=list(self._b.values()))

    def __getitem__(self, state):
        return self._b[state]
        
    def reset(self):
        """Reset belief state to uniform distribution over state space or given initial distribution
        """
        self._observation_model = self._env._observation_model
        self._state_space = self._env._state_space
        self._action_space = self._env.action_space
        self._transition_model = self._env._transition_model
        self._b = {state: 1/len(self._state_space) for state in self._state_space}

    def render(self, ax):

        state_x = [self._state_space[state][0] for state in self._state_space]
        state_y = [self._state_space[state][1] for state in self._state_space]

        maze = np.zeros((np.amax(state_x) + 2, np.amax(state_y)+2)) + -1

        for state in self._state_space:
            state_coor = self._state_space[state]
            maze[state_coor[0],state_coor[1]] = self._b[state]

        masked_maze = np.ma.masked_where(maze == -1, maze)
        cmap = copy.copy(mpl.cm.get_cmap("viridis"))
        cmap.set_bad(color='black')
        ax.imshow(masked_maze, vmin=0, vmax=1)
    
    def update(self, obs, action):
        """Bayesian update of belief state given current observation

        Args:
            obs (object): observation emitted by environment
            action (object): action performed by agent
        """

        b_prime = {}

        total_prob = 0 

        for next_state in self._state_space:
            p_obs = self._observation_model.prob(observation=obs, state=next_state)
            p_next_state = 0
            for state in self._state_space:
                p_next_state += self._b[state] * self._transition_model.prob(next_state=next_state, state=state, action=action)            
            b_prime.update({next_state: p_obs * p_next_state})
            total_prob += b_prime[next_state] 
        for s, p in b_prime.items():
            b_prime[s] = p/total_prob
        self._b = b_prime

class BeliefUpdatingRandomAgent(object):
    def __init__(self, env):
        self._env = env
        self._action_space = env.action_space
        self._belief_state = BeliefState(self._env)
    
    def reset(self):
        self._belief_state.reset()
        self._action = [0, 0]

    def render(self, ax=None):
        if ax is None:
            ax = plt.gca()
        self._belief_state.render(ax)

    def __call__(self, obs):
        """Updates belief over possible current states given current observation and previous action

        Args:
            obs (object): observation generated by environment

        Returns:
            object: action
        """
        self._belief_state.update(obs, self._action)
        self._action = self._action_space.sample()
        return self._action

class QMDPAgent(object):
    def __init__(self, env, discount_factor, horizon=15):
        self._env = env
        self._horizon = horizon
        self._lambda = discount_factor
        self._belief_state = BeliefState(self._env)

    def reset(self):
        self._belief_state.reset()
        self._state_space = self._env._state_space     
        self._transition_model = self._env._transition_model
        self._action_space = self._env._action_space
        self._reward_model = self._env._reward_model
        self._action = None
        self._lambda = 0.8
        self._Q = self._q_func()
        
    def _q_func(self):
        """Returns Q function over states
        """
        Q = {state: {action: 0 for action in self._action_space} for state in self._state_space}
        for _ in range(self._horizon):
            for state in self._state_space:
                for action in self._action_space:
                    state_prime = self._transition_model(state=state, action=action)
                    r = self._reward_model(state_prime)
                    max_q_a = np.amax([Q[state_prime][action] for action in self._action_space])
                    Q[state][action] = r + self._lambda * max_q_a
        return Q

    def render(self, ax=None):
        if ax is None: 
            ax = plt.gca()
        self._belief_state.render(ax)
    

    def info(self):
        return {'name': 'QMDPAgent', 'horizon': self._horizon, 'lambda': self._lambda}

    def __call__(self, obs):
        self._belief_state.update(obs, self._action)        
        action_vals = []
        for action in self._action_space:
            w_q_val = 0
            for state in self._state_space:
                w_q_val += self._belief_state[state] * self._Q[state][action]
            action_vals.append(w_q_val)
        self._action = np.random.choice(np.argwhere(action_vals == np.amax(action_vals)).flatten().tolist())
        return self._action


class MLSAgent(object):
    def __init__(self, env, horizon, discount_factor):
        self._env = env
        self._horizon = horizon
        self._lambda = discount_factor
        self._belief_state = BeliefState(self._env)
 
    def reset(self):
        self._belief_state.reset()
        self._state_space = self._env._state_space     
        self._transition_model = self._env._transition_model
        self._action_space = self._env._action_space
        self._reward_model = self._env._reward_model
        self._action = None
        self._Q = self._q_func()
        
    def _q_func(self):
        """Returns Q function over states
        """
        
        Q = {state: {action: 0 for action in self._action_space} for state in self._state_space}
        for _ in range(self._horizon):
            for state in self._state_space:
                for action in self._action_space:
                    state_prime = self._transition_model(state=state, action=action)
                    r = self._reward_model(state_prime)
                    max_q_a = np.amax([Q[state_prime][action] for action in self._action_space])
                    Q[state][action] = r + self._lambda * max_q_a
        return Q

    def info(self):
        return {'name': 'MLSAgent', 'horizon': self._horizon, 'lambda': self._lambda}

    def render(self, ax=None):
        if ax is None: 
            ax = plt.gca()
        self._belief_state.render(ax)
    
    def __call__(self, obs):
        self._belief_state.update(obs, self._action)
        self._mls = self._belief_state.mls()
        q_vals = np.asarray([self._Q[self._mls][action] for action in self._action_space])
        self._action = np.random.choice(np.argwhere(q_vals == np.amax(q_vals)).flatten().tolist())
        return self._action

class MLSLookupDistanceAgent(object):
    def __init__(self, env, horizon=3):
        self._env = env
        self._horizon = horizon
        self._belief_state = BeliefState(self._env)

    def reset(self):
        self._belief_state.reset()

        self._state_space = self._env._state_space     
        self._transition_func = self._env._transition_func
        self._action_space = self._env.action_space
        self._reward_model = self._env._reward_model
        self._target_state = list(filter(lambda s: self._reward_model(s) > 0, self._state_space))[0]
        self._action = [0, 0]

    def render(self, ax=None):
        if ax is None: 
            ax = plt.gca()
        self._belief_state.render(ax)
    
    def _dist(self, state):
        return np.linalg.norm(self._target_state - state)

    def __call__(self, obs):
        self._belief_state.update(obs, self._action)
        self._mls = self._belief_state.mls()
        # removing do-nothing action
        action_space = list(filter(lambda a: not np.all(self._transition_func(state=self._mls, action=a) == self._mls), self._action_space))
        # create all possible n-tuples of actions
        decision_tree = list(product(action_space, repeat = self._horizon))
        # for each n-tuple, find resulting state
        end_states = []
        cum_distances = []
        for branch in decision_tree:
            cum_dist = 0
            state = self._mls
            for action in branch:
                state = self._transition_func(action=action, state=state)
                cum_dist += self._dist(state)
            end_states.append(state)
            cum_distances.append(cum_dist)
        # choose tuple where distance from resulting state to target is minimal
        dist_minimizing_actions = decision_tree[np.argmin(cum_distances)]
        self._action = dist_minimizing_actions[np.random.choice(len(dist_minimizing_actions))]
        return self._action

class MLSDistanceAgent(object):
    def __init__(self, env):
        self._env = env
        self._belief_state = BeliefState(self._env)

    def reset(self):
        self._belief_state.reset()

        self._state_space = self._env._state_space     
        self._transition_func = self._env._transition_func
        self._action_space = self._env.action_space
        self._reward_model = self._env._reward_model
        self._target_state = list(filter(lambda s: self._reward_model(s) > 0, self._state_space))[0]
        self._action = [0, 0]
        return self._action

    def render(self, ax=None):
        if ax is None: 
            ax = plt.gca()
        self._belief_state.render(ax)
    
    def _dist(self, state):
        return np.linalg.norm(self._target_state - state)

    def __call__(self, obs):
        self._belief_state.update(obs, self._action)
        self._mls = self._belief_state.mls()
        dist = self._dist(self._mls)
        # removing actions that wouldn't change state
        action_space = list(filter(lambda a: not np.all(self._transition_func(state=self._mls, action=a) == self._mls), self._action_space))
        new_dist = [self._dist(self._transition_func(state=self._mls, action=action)) - dist for action in action_space]
        self._action = action_space[np.argmin(new_dist)]
        return self._action

class RandomAgent(object):
    def __init__(self, env):
        self._env = env
        self._action_space = self._env.action_space

    def __call__(self, obs):
        """Randomly samples from action space.

        Args:
            obs (object): observation generated by environment

        Returns:
            object: action
        """
        return self._action_space.sample()