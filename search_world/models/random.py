import numpy as np
import matplotlib.pyplot as plt

# TODO: Create agent base class with abstract methods; render, reset, __call__
# TODO: Create class for Maze states

class BeliefState(object):
    def __init__(self, env):
        self._env = env

    def mls(self):
        """Returns most likely state, breaking ties randomly. 
        """
        return self._state_space[np.random.choice(len(self._state_space), p=list(self._b.values()))]

    def reset(self):
        """Reset belief state to uniform distribution over state space or given initial distribution
        """
        self._observation_model = self._env._observation_model
        self._state_space = self._env._state_space
        self._state_mapping = {str(state): state for state in self._state_space}
        self._action_space = self._env.action_space
        self._transition_model = self._env._transition_model
        self._b = {str(state): 1/len(self._state_space) for state in self._state_space}

    def render(self, ax):
        state_x = [state[0] for state in self._state_space]
        state_y = [state[1] for state in self._state_space]

        maze = np.zeros((np.amax(state_x) + 2, np.amax(state_y)+2))
        for state in self._state_space:
            maze[state[0],state[1]] = self._b[str(state)]

        ax.imshow(maze, vmin=0, vmax=1)
    
    def update(self, obs, action):
        """Bayesian update of belief state given current observation

        Args:
            obs (object): observation emitted by environment
            action (object): action performed by agent
        """
        b_prime = {}

        total_prob = 0 
        for next_state in self._state_space:
            p_obs = self._observation_model(observation=obs, state=next_state)
            p_next_state = 0
            for state in self._state_space:
                p_next_state += self._b[str(state)] * self._transition_model(next_state=next_state, state=state, action=action)            
            b_prime.update({str(next_state): p_obs * p_next_state})
            total_prob += b_prime[str(next_state)] 
        for s, p in b_prime.items():
            b_prime[s] = p/total_prob

        self._b = b_prime

class BeliefUpdatingRandomAgent(object):
    def __init__(self, env):
        self._env = env
        self._action_space = env.action_space
        self._belief_state = BeliefState(self._env)
    
    def reset(self):
        self._belief_state.reset()
        self._action = [0, 0]

    def render(self, ax=None):
        if ax is None:
            ax = plt.gca()
        self._belief_state.render(ax)

    def __call__(self, obs):
        """Updates belief over possible current states given current observation and previous action

        Args:
            obs (object): observation generated by environment

        Returns:
            object: action
        """
        self._belief_state.update(obs, self._action)
        self._action = self._action_space.sample()
        return self._action

class MLSDistanceAgent(object):
    def __init__(self, env):
        self._env = env
        self._belief_state = BeliefState(self._env)

    def reset(self):
        self._belief_state.reset()
        # TODO: Make state space an accessible property of environment; part of wrapper
        self._state_space = self._env._state_space     
        self._transition_func = self._env._transition_func
        self._action_space = self._env.action_space
        self._reward_model = self._env._reward_model
        self._target_state = list(filter(lambda s: self._reward_model(s) > 0, self._state_space))[0]
        self._action = [0, 0]

    def render(self, ax=None):
        if ax is None: 
            ax = plt.gca()
        self._belief_state.render(ax)
    
    def _dist(self, state):
        return self._target_state - state

    def __call__(self, obs):
        self._belief_state.update(obs, self._action)
        self._mls = self._belief_state.mls()
        new_dist = [np.linalg.norm(self._dist(self._transition_func(state=self._mls, action=action))) for action in self._action_space]
        self._action = self._action_space[np.argmin(new_dist)]
        return self._action

class RandomAgent(object):
    def __init__(self, env):
        self._env = env
        self._action_space = self._env.action_space

    def __call__(self, obs):
        """Randomly samples from action space.

        Args:
            obs (object): observation generated by environment

        Returns:
            object: action
        """
        return self._action_space.sample()
